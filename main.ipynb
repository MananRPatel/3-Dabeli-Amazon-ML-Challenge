{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas\n",
    "!pip install flash_attn timm transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkwE7KOq-XWU",
    "outputId": "00f8a888-a18e-4f4f-8204-fafeab953658"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base-ft\", trust_remote_code=True, revision='refs/pr/6').to(device)\n",
    "\n",
    "model = torch.nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base-ft\", trust_remote_code=True, revision='refs/pr/6')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LMQSuBLIqiVN",
    "outputId": "685c3e6f-4524-4355-8cb1-8cf14a7f05f7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "from datasets import Dataset\n",
    "from utils import download_images\n",
    "\n",
    "train_csv_path = 'dataset/train.csv'\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "\n",
    "test_csv_path = 'dataset/test.csv'\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "download_images(train_df['image_link'], 'dataset/trains')\n",
    "download_images(test_df['image_link'], 'dataset/tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "target_distribution = {\n",
    "    'height': 2000,\n",
    "    'width': 2000,\n",
    "    'depth': 2000,\n",
    "    'item_weight': 2000,\n",
    "    'maximum_weight_recommendation': 500,\n",
    "    'wattage': 500,\n",
    "    'voltage': 500,\n",
    "    'item_volume': 500\n",
    "}\n",
    "\n",
    "# Create an empty DataFrame to store the balanced dataset\n",
    "balanced_df = pd.DataFrame()\n",
    "\n",
    "# Downsample each entity group to the specified number\n",
    "for entity_name, target_count in target_distribution.items():\n",
    "    # Filter the data for the current entity\n",
    "    entity_data = train_df[train_df['entity_name'] == entity_name]\n",
    "    \n",
    "    # If the number of examples is greater than the target, downsample\n",
    "    if len(entity_data) > target_count:\n",
    "        entity_data = entity_data.sample(n=target_count, random_state=42)\n",
    "    \n",
    "    # Append the sampled data to the balanced DataFrame\n",
    "    balanced_df = pd.concat([balanced_df, entity_data])\n",
    "\n",
    "# Optionally, reset the index of the balanced DataFrame\n",
    "balanced_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_df = balanced_df\n",
    "print(train_df['entity_name'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process train and test datasets\n",
    "df_train = Dataset.from_pandas(train_df)\n",
    "df_test = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "61SSqMfp_FZ_"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "entity_unit_map = {\n",
    "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'item_weight': {'gram',\n",
    "        'kilogram',\n",
    "        'microgram',\n",
    "        'milligram',\n",
    "        'ounce',\n",
    "        'pound',\n",
    "        'ton'},\n",
    "    'maximum_weight_recommendation': {'gram',\n",
    "        'kilogram',\n",
    "        'microgram',\n",
    "        'milligram',\n",
    "        'ounce',\n",
    "        'pound',\n",
    "        'ton'},\n",
    "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
    "    'wattage': {'kilowatt', 'watt'},\n",
    "    'item_volume': {'centilitre',\n",
    "        'cubic foot',\n",
    "        'cubic inch',\n",
    "        'cup',\n",
    "        'decilitre',\n",
    "        'fluid ounce',\n",
    "        'gallon',\n",
    "        'imperial gallon',\n",
    "        'litre',\n",
    "        'microlitre',\n",
    "        'millilitre',\n",
    "        'pint',\n",
    "        'quart'}\n",
    "}\n",
    "\n",
    "\n",
    "class DocVQADataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        my_list = entity_unit_map[example['entity_name']]\n",
    "        result = ' '.join(my_list)  \n",
    "        question = \"<VQA> What is the \" + example['entity_name'] + \" of this entity ? answer should be in this list of units only \"+ result\n",
    "        answer = example['entity_value']\n",
    "        filename = example['image_link'].split('/')[-1]\n",
    "        image_filename = os.path.basename(filename)\n",
    "        image_path = os.path.join(\"dataset/trains/\", image_filename)\n",
    "\n",
    "        if os.path.exists(image_path):\n",
    "            image = Image.open(image_path)\n",
    "        else:\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        return question, answer, image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('florence model weight epoch 4.pth'))\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Fz6vHuHPABTz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (AdamW, AutoProcessor, get_scheduler)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    questions, answers, images = zip(*batch)\n",
    "    inputs = processor(text=list(questions), images=list(images), return_tensors=\"pt\", padding=True).to(device)\n",
    "    return inputs, answers\n",
    "\n",
    "# Create datasets\n",
    "\n",
    "train_dataset = DocVQADataset(df_train)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 1\n",
    "num_workers = 0\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, num_workers=num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kadu96TXARtm",
    "outputId": "99b3e7e2-1cc3-4d58-f29c-1779cdf681ce"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "def train_model(train_loader, model, processor, epochs=10, lr=1e-6):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    num_training_steps = epochs * len(train_loader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\", leave=True)\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            inputs, answers = batch\n",
    "\n",
    "            input_ids = inputs[\"input_ids\"].to(device)\n",
    "            pixel_values = inputs[\"pixel_values\"].to(device)\n",
    "            labels = processor.tokenizer(text=answers, return_tensors=\"pt\", padding=True, return_token_type_ids=False).input_ids.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss.mean()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Accumulate the loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Update tqdm progress bar with loss\n",
    "            progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "        # Compute the average loss for the epoch\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Save model checkpoint\n",
    "        output_dir = f\"./model_checkpoints/epoch_{epoch+1}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        model.module.save_pretrained(output_dir)  # Save model in a multi-GPU environment\n",
    "        processor.save_pretrained(output_dir)\n",
    "\n",
    "# Freezing vision tower parameters (you may skip if you want to fine-tune the whole model)\n",
    "for param in model.module.vision_tower.parameters():  # Use model.module for DataParallel\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Train the model\n",
    "train_model(train_loader, model, processor, epochs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8yY-qJd2AceH"
   },
   "outputs": [],
   "source": [
    "def run_example(task_prompt, text_input, image):\n",
    "    prompt = task_prompt + text_input\n",
    "\n",
    "    # Ensure the image is in RGB mode\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    # Prepare inputs\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Forward pass through DataParallel model\n",
    "    with torch.no_grad():  # No need to track gradients during inference\n",
    "        # Access the underlying model from DataParallel\n",
    "        model_for_generation = model.module if hasattr(model, 'module') else model\n",
    "\n",
    "        # Generate output\n",
    "        generated_ids = model_for_generation.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            pixel_values=inputs[\"pixel_values\"],\n",
    "            max_new_tokens=1024,\n",
    "            num_beams=3\n",
    "        )\n",
    "\n",
    "    # Decode generated text\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    \n",
    "    # Post-process the generated text\n",
    "    parsed_answer = processor.post_process_generation(generated_text, task=task_prompt, image_size=(image.width, image.height))\n",
    "    \n",
    "    return parsed_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Existing list of lists\n",
    "data = [\n",
    "]\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display  # For displaying images in Jupyter environments\n",
    "\n",
    "def process_examples(dataset, run_example_func):\n",
    "    # Add tqdm to show a progress bar\n",
    "    for idx in tqdm(range(len(dataset)), desc=\"Processing examples\"):\n",
    "        example = dataset[idx]  # Direct indexing for Dataset object\n",
    "        # Ensure image_link is a key in your dataset\n",
    "        filename = example['image_link'].split('/')[-1]\n",
    "        image_filename = os.path.basename(filename)\n",
    "        image_path = os.path.join(\"dataset/tests/\", image_filename)\n",
    "\n",
    "        if os.path.exists(image_path):\n",
    "            image = Image.open(image_path)\n",
    "        else:\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "            continue  # Skip to the next iteration if the image is not found\n",
    "\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "\n",
    "        my_list = entity_unit_map[example['entity_name']]\n",
    "        result = ' '.join(my_list)  \n",
    "\n",
    "        #print(result)\n",
    "        \n",
    "        question = \"<VQA> What is the \" + example['entity_name'] + \" of this entity ? answer should be in this list of units only \"+ result\n",
    "        answer = run_example_func(\"VQA\", question, image)\n",
    "        \n",
    "        data.append([example['index'], answer['VQA']])\n",
    "\n",
    "        if(idx%10000==0):\n",
    "            \n",
    "            # Convert the updated list to a DataFrame\n",
    "            df = pd.DataFrame(data, columns=['index', 'entity_value'])\n",
    "            \n",
    "            # Save the DataFrame to a CSV file\n",
    "            df.to_csv(f'index_value2_{idx}.csv', index=False)\n",
    "                    \n",
    "        # Display the image if you're in a Jupyter notebook or similar environment\n",
    "        #display(image)  # Ensure 'display' is imported if not in Jupyter, you might use another method\n",
    "\n",
    "# Call the function\n",
    "process_examples(df_test, run_example)\n",
    "\n",
    "\n",
    "# Convert the updated list to a DataFrame\n",
    "df = pd.DataFrame(data, columns=['index', 'entity_value'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('result.csv', index=False)\n",
    "\n",
    "print(\"Updated CSV file 'result.csv' generated successfully\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
